#!/usr/bin/env python3
"""
AI Research Paper Fetcher
Fetches state-of-the-art AI research papers from arXiv, scores by relevance and novelty,
summarizes key innovations with GPT-4o-mini, and creates Notion entries for automated posting.
Focuses on cs.AI and cs.LG categories for cutting-edge ML/AI research.
"""

import os
import sys
import json
import logging
import argparse
import time
import re
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Optional, Set, Tuple
from collections import Counter

import arxiv
import requests
from dateutil import parser as date_parser
from notion_client import Client
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Optional OpenAI integration
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# Optional PDF parsing
try:
    from PyPDF2 import PdfReader
    from io import BytesIO
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    logging.warning("PyPDF2 not available - will use abstracts only")

# ----- Config -----
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
NOTION_DB_ID = os.getenv("NOTION_DB_ID")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# arXiv search parameters
ARXIV_CATEGORIES = ["cs.AI", "cs.LG", "cs.CL", "cs.CV"]  # AI, ML, NLP, Computer Vision
ARXIV_MAX_RESULTS = 100  # Fetch more to score and filter
ARXIV_MAX_AGE_DAYS = 7  # Only papers from last 7 days

# Research-focused keywords (SOTA, innovations, methods)
BOOST_KEYWORDS = [
    "state-of-the-art", "SOTA", "novel", "breakthrough", "outperforms",
    "large language model", "LLM", "GPT", "transformer", "diffusion",
    "reinforcement learning", "RL", "neural network", "deep learning",
    "fine-tuning", "RLHF", "instruction", "reasoning", "emergent",
    "multimodal", "vision-language", "agents", "planning"
]

# Innovation indicators (higher weight)
INNOVATION_KEYWORDS = [
    "state-of-the-art", "SOTA", "novel", "breakthrough", "first",
    "outperforms", "surpasses", "exceeds", "achieves", "improves"
]

MAX_ARTICLE_AGE_HOURS = 168  # 7 days in hours
RECENCY_BOOST_HOURS = 24

# Platform-specific character limits
MAX_X_CHARS = 280  # X (Twitter) character limit
MAX_LINKEDIN_CHARS = 2000  # LinkedIn optimal limit (3000 max, 2000 best for engagement)
SUMMARY_MAX_CHARS = 220  # Legacy fallback summary length

# ----- Logging -----
# Configure logging level from environment variable (default: INFO)
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
numeric_level = getattr(logging, log_level, logging.INFO)

logging.basicConfig(
    level=numeric_level,
    format="%(asctime)s [%(levelname)s] [%(filename)s:%(lineno)d] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)
logger.debug(f"Logging initialized at level: {log_level}")

# ----- Data Models -----
class ResearchPaper:
    """Data model for AI research papers from arXiv."""
    def __init__(
        self,
        title: str,
        arxiv_id: str,
        pdf_url: str,
        published: datetime,
        authors: List[str],
        abstract: str,
        categories: List[str],
        primary_category: str = None
    ):
        self.title = title
        self.arxiv_id = arxiv_id
        self.pdf_url = pdf_url
        self.published = published
        self.authors = authors
        self.abstract = abstract
        self.categories = categories
        self.primary_category = primary_category or (categories[0] if categories else "cs.AI")
        self.score = 0.0
        self.full_text = None  # Optional: extracted PDF text

    def __repr__(self):
        return f"<ResearchPaper '{self.title[:50]}...' arXiv:{self.arxiv_id} score={self.score:.2f}>"


# ----- arXiv Paper Fetching -----
def fetch_arxiv_papers() -> List[ResearchPaper]:
    """
    Fetch recent AI/ML research papers from arXiv.
    Queries cs.AI, cs.LG, cs.CL, cs.CV categories for papers from last 7 days.
    """
    logger.info(f"üîç Fetching papers from arXiv (categories: {', '.join(ARXIV_CATEGORIES)})")
    
    # Build search query for multiple categories
    category_query = " OR ".join([f"cat:{cat}" for cat in ARXIV_CATEGORIES])
    
    logger.debug(f"arXiv query: {category_query}")
    logger.debug(f"Max results: {ARXIV_MAX_RESULTS}, Max age: {ARXIV_MAX_AGE_DAYS} days")
    
    papers = []
    cutoff = datetime.now(timezone.utc) - timedelta(days=ARXIV_MAX_AGE_DAYS)
    
    try:
        # Create arXiv client and search
        client = arxiv.Client()
        search = arxiv.Search(
            query=category_query,
            max_results=ARXIV_MAX_RESULTS,
            sort_by=arxiv.SortCriterion.SubmittedDate,  # Most recent first
            sort_order=arxiv.SortOrder.Descending
        )
        
        logger.info(f"üì° Querying arXiv API...")
        
        for result in client.results(search):
            # Filter by age
            if result.published < cutoff:
                logger.debug(f"Skipping old paper: {result.title[:50]}... ({result.published})")
                continue
            
            # Extract data
            paper = ResearchPaper(
                title=result.title.strip(),
                arxiv_id=result.entry_id.split('/')[-1],  # Extract ID from URL
                pdf_url=result.pdf_url,
                published=result.published,
                authors=[author.name for author in result.authors],
                abstract=result.summary.strip(),
                categories=result.categories,
                primary_category=result.primary_category
            )
            
            papers.append(paper)
            logger.debug(f"Added paper: {paper.arxiv_id} - {paper.title[:60]}...")
        
        logger.info(f"‚úÖ Fetched {len(papers)} papers from last {ARXIV_MAX_AGE_DAYS} days")
        
        if papers:
            logger.info(f"üìä Date range: {min(p.published for p in papers)} to {max(p.published for p in papers)}")
        
        return papers
    
    except Exception as e:
        logger.error(f"‚ùå Failed to fetch arXiv papers: {e}", exc_info=True)
        return []


def extract_pdf_text(pdf_url: str, max_chars: int = 3000) -> Optional[str]:
    """
    Download and extract text from arXiv PDF (first few pages).
    Used for richer summarization context beyond abstract.
    """
    if not PDF_AVAILABLE:
        logger.debug("PyPDF2 not available, skipping PDF extraction")
        return None
    
    try:
        logger.debug(f"üìÑ Downloading PDF from {pdf_url}...")
        
        response = requests.get(pdf_url, timeout=30, stream=True)
        response.raise_for_status()
        
        # Read PDF from bytes
        pdf_file = BytesIO(response.content)
        reader = PdfReader(pdf_file)
        
        # Extract text from first 3 pages (introduction usually)
        text_parts = []
        max_pages = min(3, len(reader.pages))
        
        for i in range(max_pages):
            page_text = reader.pages[i].extract_text()
            if page_text:
                text_parts.append(page_text)
        
        full_text = " ".join(text_parts)
        
        # Clean up text
        full_text = re.sub(r'\s+', ' ', full_text).strip()
        
        # Truncate to max chars
        if len(full_text) > max_chars:
            full_text = full_text[:max_chars]
        
        logger.debug(f"‚úÖ Extracted {len(full_text)} chars from PDF")
        return full_text
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Failed to extract PDF text: {e}")
        return None
def get_recent_notion_content(notion: Client, db_id: str, days: int = 7) -> Set[Tuple[str, str]]:
    """
    Query Notion for recent Posted/Scheduled/Failed entries to prevent duplicates.
    Returns set of (normalized_title, arxiv_id) tuples.
    """
    try:
        cutoff = datetime.now(timezone.utc) - timedelta(days=days)
        cutoff_iso = cutoff.replace(microsecond=0).isoformat().replace('+00:00', 'Z')
        
        # Query for entries from last N days
        results = notion.databases.query(
            database_id=db_id,
            filter={
                "or": [
                    {"property": "Status", "select": {"equals": "Posted"}},
                    {"property": "Status", "select": {"equals": "Scheduled"}},
                    {"property": "Status", "select": {"equals": "Failed"}},
                ],
                "and": [
                    {"property": "Scheduled Time", "date": {"after": cutoff_iso}}
                ]
            },
            page_size=100
        )
        
        seen_content = set()
        for page in results.get("results", []):
            # Extract title from Title property
            title_prop = page["properties"].get("Title", {})
            title_blocks = title_prop.get("title", [])
            if title_blocks:
                content = "".join(b.get("plain_text", "") for b in title_blocks)
                # Normalize: lowercase, strip, remove extra spaces
                normalized = " ".join(content.lower().strip().split())
                if normalized and not normalized.startswith("[error]"):
                    # Try to extract arXiv ID if present
                    arxiv_id = ""
                    arxiv_prop = page["properties"].get("arXiv ID", {})
                    if arxiv_prop:
                        arxiv_text = arxiv_prop.get("rich_text", [])
                        if arxiv_text:
                            arxiv_id = "".join(b.get("plain_text", "") for b in arxiv_text)
                    
                    seen_content.add((normalized, arxiv_id))
        
        logger.info(f"Found {len(seen_content)} recent entries in Notion (last {days} days)")
        return seen_content
    
    except Exception as e:
        logger.warning(f"Failed to query recent Notion content: {e}")
        return set()


def normalize_title(title: str) -> str:
    """Normalize title for comparison."""
    return " ".join(title.lower().strip().split())


def title_similarity(title1: str, title2: str) -> float:
    """Calculate simple word overlap similarity between titles."""
    words1 = set(normalize_title(title1).split())
    words2 = set(normalize_title(title2).split())
    
    if not words1 or not words2:
        return 0.0
    
    intersection = words1 & words2
    union = words1 | words2
    
    return len(intersection) / len(union) if union else 0.0


def score_papers(papers: List[ResearchPaper], notion_recent: Optional[Set[Tuple[str, str]]] = None) -> List[ResearchPaper]:
    """
    Score research papers by relevance, novelty, and recency.
    Returns sorted list (highest score first).
    
    Scoring factors:
    - Recency: <24h +20pts, 24-48h +15pts, 48-96h +10pts, 96-168h +5pts
    - Innovation keywords: +5pts each (SOTA, novel, breakthrough, etc.)
    - General keywords: +2pts each (LLM, transformer, etc.)
    - Category match: +3pts for cs.AI/cs.LG primary
    - Abstract length: +2pts if >500 chars (detailed paper)
    - Duplicate penalty: -50pts for similar titles in Notion
    """
    logger.info(f"üìä Scoring {len(papers)} papers...")
    
    now = datetime.now(timezone.utc)
    filtered_papers = []
    
    for paper in papers:
        score = 0.0
        
        # Calculate age in hours
        age_h = (now - paper.published).total_seconds() / 3600
        
        # Recency boost (research is time-sensitive)
        if 0 <= age_h <= 24:
            score += 20  # Brand new (< 24h)
        elif age_h <= 48:
            score += 15  # Very fresh (24-48h)
        elif age_h <= 96:
            score += 10  # Fresh (48-96h / 2-4 days)
        elif age_h <= 168:
            score += 5   # Recent (96-168h / 4-7 days)
        else:
            score -= 100  # Too old (shouldn't happen with 7-day filter)
        
        # Innovation keyword matching (high weight)
        title_abstract = (paper.title + " " + paper.abstract).upper()
        innovation_count = sum(1 for kw in INNOVATION_KEYWORDS if kw.upper() in title_abstract)
        score += innovation_count * 5
        
        if innovation_count > 0:
            logger.debug(f"Innovation keywords found: {innovation_count} in {paper.title[:40]}...")
        
        # General keyword matching
        general_count = sum(1 for kw in BOOST_KEYWORDS if kw.upper() in title_abstract)
        score += general_count * 2
        
        # Category boost (prefer core AI/ML)
        if paper.primary_category in ["cs.AI", "cs.LG"]:
            score += 3
            logger.debug(f"Category boost for {paper.primary_category}: {paper.title[:40]}...")
        
        # Abstract quality (longer abstracts usually mean more detailed papers)
        if len(paper.abstract) > 500:
            score += 2
        
        # Check similarity with recent Notion content
        if notion_recent:
            norm_title = normalize_title(paper.title)
            for notion_title, notion_arxiv_id in notion_recent:
                # Check arXiv ID match (exact duplicate)
                if notion_arxiv_id and notion_arxiv_id == paper.arxiv_id:
                    penalty = 100  # Heavy penalty for exact duplicate
                    score -= penalty
                    logger.debug(f"Exact arXiv ID match: -{penalty} for {paper.arxiv_id}")
                    break
                
                # Check title similarity
                similarity = title_similarity(norm_title, notion_title)
                if similarity > 0.6:  # High similarity threshold
                    penalty = (similarity - 0.6) * 25  # Strong penalty
                    score -= penalty
                    logger.debug(f"Title similarity penalty: -{penalty:.2f} for '{paper.title[:40]}...'")
                    break
        
        paper.score = score
        filtered_papers.append(paper)
    
    # Sort by score descending
    filtered_papers.sort(key=lambda x: x.score, reverse=True)
    
    logger.info(f"‚úÖ Scored and sorted {len(filtered_papers)} papers")
    
    # Log top 5 for debugging
    if filtered_papers:
        logger.info("üèÜ Top 5 scored papers:")
        for i, paper in enumerate(filtered_papers[:5], 1):
            age_h = (now - paper.published).total_seconds() / 3600
            logger.info(f"  {i}. [{paper.score:.2f}] {paper.title[:70]}...")
            logger.info(f"     arXiv:{paper.arxiv_id}, {paper.primary_category}, {age_h:.1f}h old")
    
    return filtered_papers
    """
    Fetch and parse all RSS feeds, returning normalized NewsItem objects.
    Enhanced with:
    - Notion duplicate checking
    - Exponential backoff retries
    - Better deduplication
    """
    items = []
    seen = set()  # dedupe by (link, normalized_title)
    
    # Get recent Notion content to prevent duplicates
    notion_seen = set()
    if NOTION_TOKEN and NOTION_DB_ID:
        try:
            notion = Client(auth=NOTION_TOKEN)
            notion_seen = get_recent_notion_content(notion, NOTION_DB_ID, days=7)
        except Exception as e:
            logger.warning(f"Could not fetch Notion history: {e}")
    
    for feed_url in RSS_FEEDS:
        max_retries = 3
        for attempt in range(max_retries):
            try:
                logger.info(f"Fetching feed: {feed_url} (attempt {attempt + 1}/{max_retries})")
                response = requests.get(feed_url, timeout=15)
                response.raise_for_status()
                feed = feedparser.parse(response.content)
                
                if feed.bozo and not feed.entries:
                    logger.warning(f"Feed parsing issue for {feed_url}: {feed.bozo_exception}")
                    break  # Don't retry on parse errors
                
                for entry in feed.entries:
                    title = entry.get("title", "").strip()
                    link = entry.get("link", "").strip()
                    
                    if not title or not link:
                        continue
                    
                    # Normalize title for comparison
                    norm_title = normalize_title(title)
                    
                    # Dedupe by link and normalized title
                    dedupe_key = (link, norm_title)
                    if dedupe_key in seen:
                        continue
                    
                    # Check against recent Notion content
                    if any(title_similarity(norm_title, n[0]) > 0.7 for n in notion_seen):
                        logger.warning(f"Skipping duplicate from Notion history: {title[:60]}...")
                        continue
                    
                    seen.add(dedupe_key)
                    
                    # Parse published date
                    published_str = entry.get("published") or entry.get("updated")
                    if published_str:
                        try:
                            published = date_parser.parse(published_str)
                            # Make timezone-aware if naive
                            if published.tzinfo is None:
                                published = published.replace(tzinfo=timezone.utc)
                        except Exception:
                            published = datetime.now(timezone.utc)
                    else:
                        published = datetime.now(timezone.utc)
                    
                    # Extract domain
                    extracted = tldextract.extract(link)
                    source_domain = f"{extracted.domain}.{extracted.suffix}" if extracted.domain else "unknown"
                    
                    # Image URL (optional)
                    image_url = None
                    if "media_content" in entry and entry.media_content:
                        image_url = entry.media_content[0].get("url")
                    elif "media_thumbnail" in entry and entry.media_thumbnail:
                        image_url = entry.media_thumbnail[0].get("url")
                    elif "enclosures" in entry and entry.enclosures:
                        for enc in entry.enclosures:
                            if enc.get("type", "").startswith("image"):
                                image_url = enc.get("href")
                                break
                    
                    # Summary (for fallback)
                    summary = entry.get("summary", "")
                    
                    items.append(NewsItem(
                        title=title,
                        link=link,
                        published=published,
                        source_domain=source_domain,
                        image_url=image_url,
                        summary=summary
                    ))
                
                # Success - break retry loop
                break
                
            except requests.RequestException as e:
                logger.warning(f"Request error for {feed_url} (attempt {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    sleep_time = 5 * (2 ** attempt)  # Exponential backoff: 5s, 10s, 20s
                    logger.info(f"Retrying in {sleep_time}s...")
                    time.sleep(sleep_time)
                else:
                    logger.error(f"Failed to fetch {feed_url} after {max_retries} attempts")
            except Exception as e:
                logger.error(f"Unexpected error fetching feed {feed_url}: {e}")
                break  # Don't retry on unexpected errors
    
    logger.info(f"Parsed {len(items)} unique items from {len(RSS_FEEDS)} feeds")
    return items


# ----- Scoring -----
def score_items(items: List[NewsItem], notion_recent: Optional[Set[Tuple[str, str]]] = None) -> List[NewsItem]:
    """
    Score items based on recency, keyword matches, and source diversity.
    Enhanced with:
    - Source diversity penalties (prevent over-representation)
    - Stricter recency boost (<24h heavily favored)
    - Similarity check against recent Notion content
    Filter out items older than 48h.
    """
    now = datetime.now(timezone.utc)
    cutoff = now - timedelta(hours=MAX_ARTICLE_AGE_HOURS)
    recent_boost_cutoff = now - timedelta(hours=RECENCY_BOOST_HOURS)
    
    # Count source distribution
    source_counter = Counter(item.source_domain for item in items)
    total_items = len(items)
    
    filtered_items = []
    
    for item in items:
        # Filter out old items
        if item.published < cutoff:
            continue
        
        score = 0.0
        
        # Enhanced recency boost with stricter penalties
        if item.published:
            age_h = (now - item.published).total_seconds() / 3600
            if 0 <= age_h <= 6:
                score += 15  # Super fresh (< 6h)
            elif age_h <= 12:
                score += 12  # Very fresh (6-12h)
            elif age_h <= 24:
                score += 8   # Fresh (12-24h)
            elif age_h <= 36:
                score += 3   # Recent (24-36h)
            elif age_h <= 48:
                score += 1   # Older (36-48h) - minimal boost
            else:
                score -= 100  # Too old
        
        # Keyword matching: +2 per keyword in title
        title_upper = item.title.upper()
        for keyword in BOOST_KEYWORDS:
            if keyword.upper() in title_upper:
                score += 2.0
        
        # Source diversity penalty: penalize over-represented sources
        source_freq = source_counter[item.source_domain] / total_items
        if source_freq > 0.5:  # If source has >50% of items
            penalty = (source_freq - 0.5) * 10  # Penalty scales with over-representation
            score -= penalty
            logger.debug(f"Applied diversity penalty -{penalty:.2f} to {item.source_domain}")
        elif source_freq > 0.3:  # Moderate over-representation
            penalty = (source_freq - 0.3) * 5
            score -= penalty
        
        # Check similarity with recent Notion content if provided
        if notion_recent:
            norm_title = normalize_title(item.title)
            for notion_title, _ in notion_recent:
                similarity = title_similarity(norm_title, notion_title)
                if similarity > 0.6:  # High similarity threshold
                    penalty = (similarity - 0.6) * 15  # Strong penalty for similar content
                    score -= penalty
                    logger.debug(f"Applied similarity penalty -{penalty:.2f} for '{item.title[:40]}...'")
                    break
        
        item.score = score
        filtered_items.append(item)
    
    # Sort by score descending
    filtered_items.sort(key=lambda x: x.score, reverse=True)
    
    logger.info(f"Scored and filtered to {len(filtered_items)} items (within {MAX_ARTICLE_AGE_HOURS}h)")
    
    # Log top 5 for debugging
    if filtered_items:
        logger.info("Top 5 scored items:")
        for i, item in enumerate(filtered_items[:5], 1):
            age_h = (now - item.published).total_seconds() / 3600
            logger.info(f"  {i}. [{item.score:.2f}] {item.title[:60]}... ({item.source_domain}, {age_h:.1f}h old)")
    
    return filtered_items


# ----- Summarization -----
def summarize_with_openai_dual(title: str, link: str, domain: str) -> Dict[str, Any]:
    """
    Use OpenAI to generate platform-specific summaries for X and LinkedIn.
    Returns dict with: {"x_text": str, "linkedin_text": str, "char_counts": dict}
    """
    logger.debug(f"summarize_with_openai_dual() called for: {title[:50]}...")
    
    if not OPENAI_AVAILABLE or not OPENAI_API_KEY:
        logger.error("OpenAI not available or API key missing")
        raise RuntimeError("OpenAI not available")
    
    # Initialize client
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        logger.debug(f"OpenAI client initialized with model: {OPENAI_MODEL}")
    except Exception as e:
        logger.error(f"Failed to initialize OpenAI client: {e}", exc_info=True)
        raise
    
    # Enhanced prompt for dual-platform content generation
    sys_msg = """You are an expert AI content curator for Algorythmos, a France-based AI automation firm. Summarize the following article for social posting. Generate TWO versions:

1. **X (Twitter) Version**: ‚â§280 characters total (including spaces, emojis, hashtags). Make it punchy, engaging, with a hook, 1-2 key insights, a question or CTA, and end with the link. Use 1-2 relevant hashtags (#AI, #MachineLearning). Keep it concise and impactful.

2. **LinkedIn Version**: ‚â§2000 characters total. Professional tone for AI professionals: Start with a compelling hook, include 3-5 bullet takeaways, brief analysis/implications for enterprises, CTA to discuss or connect, and the link. Use 2-3 hashtags. Include emojis sparingly for readability. Make it informative and valuable.

Output STRICTLY as valid JSON (no extra text, no markdown code blocks):
{
  "x_text": "Full X post text here (‚â§280 chars)",
  "linkedin_text": "Full LinkedIn post text here (‚â§2000 chars)",
  "char_counts": {
    "x": [exact character count],
    "linkedin": [exact character count]
  }
}

Guidelines:
- Focus on AI innovation, business impact, and relevance to Algorythmos
- Be original‚Äîrephrase insights, don't just copy
- Ensure neutral, positive, professional tone
- If article details are limited, infer key points logically
- Do NOT include the URL in character counts (it will be appended separately)
- Keep X version under 280 chars, LinkedIn version under 2000 chars"""
    
    user_msg = f"""Article Details:
Title: {title}
Link: {link}
Source: {domain}

Generate engaging social media content for both X (Twitter) and LinkedIn."""
    
    logger.debug(f"Sending request to OpenAI - Model: {OPENAI_MODEL}")
    logger.debug(f"User message: {user_msg[:200]}...")
    
    try:
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": sys_msg},
                {"role": "user", "content": user_msg},
            ],
            temperature=0.7,  # Higher temperature for more creative content
            max_tokens=1500,  # Increased for longer LinkedIn posts
            response_format={"type": "json_object"}  # Force JSON response
        )
        
        logger.debug(f"OpenAI API response received - Usage: {resp.usage}")
        raw_response = (resp.choices[0].message.content or "").strip()
        logger.debug(f"Raw OpenAI response: {raw_response[:500]}...")
        
        # Parse JSON response
        try:
            result = json.loads(raw_response)
            logger.debug(f"Parsed JSON successfully - Keys: {list(result.keys())}")
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse OpenAI JSON response: {e}")
            logger.error(f"Raw response: {raw_response}")
            raise RuntimeError(f"Invalid JSON from OpenAI: {e}")
        
        # Extract and validate texts
        x_text = result.get("x_text", "").strip()
        linkedin_text = result.get("linkedin_text", "").strip()
        char_counts = result.get("char_counts", {})
        
        if not x_text or not linkedin_text:
            logger.error(f"Missing texts in OpenAI response: x_text={bool(x_text)}, linkedin_text={bool(linkedin_text)}")
            raise RuntimeError("OpenAI did not return both platform texts")
        
        # Validate and truncate if needed
        original_x_len = len(x_text)
        original_linkedin_len = len(linkedin_text)
        
        if len(x_text) > MAX_X_CHARS:
            logger.warning(f"‚ö†Ô∏è X text exceeded limit ({len(x_text)} > {MAX_X_CHARS}), truncating...")
            x_text = x_text[:MAX_X_CHARS - 3] + "..."
        
        if len(linkedin_text) > MAX_LINKEDIN_CHARS:
            logger.warning(f"‚ö†Ô∏è LinkedIn text exceeded limit ({len(linkedin_text)} > {MAX_LINKEDIN_CHARS}), truncating...")
            linkedin_text = linkedin_text[:MAX_LINKEDIN_CHARS - 3] + "..."
        
        logger.info(f"‚úÖ Generated dual-platform content:")
        logger.info(f"   üê¶ X: {len(x_text)}/{MAX_X_CHARS} chars (original: {original_x_len})")
        logger.info(f"   üíº LinkedIn: {len(linkedin_text)}/{MAX_LINKEDIN_CHARS} chars (original: {original_linkedin_len})")
        logger.debug(f"X text: {x_text}")
        logger.debug(f"LinkedIn text preview: {linkedin_text[:200]}...")
        
        return {
            "x_text": x_text,
            "linkedin_text": linkedin_text,
            "char_counts": {
                "x": len(x_text),
                "linkedin": len(linkedin_text)
            }
        }
    
    except Exception as e:
        logger.error(f"OpenAI API error: {type(e).__name__}: {e}", exc_info=True)
        raise


def summarize_fallback(item: NewsItem) -> Dict[str, Any]:
    """
    Fallback summarization when OpenAI is unavailable.
    Returns dict with x_text and linkedin_text.
    """
    logger.debug(f"summarize_fallback() called for: {item.title[:50]}...")
    import re
    
    # Start with title
    base_summary = item.title
    
    # Try to extract key content from summary
    if item.summary:
        clean_summary = re.sub(r'<[^>]+>', '', item.summary)
        clean_summary = re.sub(r'\s+', ' ', clean_summary).strip()
        
        sentences = clean_summary.split(". ")
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 30:
                continue
            if any(kw.lower() in sentence.lower() for kw in BOOST_KEYWORDS):
                base_summary = sentence
                break
        else:
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) >= 30:
                    base_summary = sentence
                    break
    
    domain_suffix = f" ({item.source_domain})"
    
    # Generate X version (short)
    x_max_len = MAX_X_CHARS - len(domain_suffix) - len(" #AI ") - 30  # Reserve space for hashtag and link
    if len(base_summary) > x_max_len:
        x_text = base_summary[:x_max_len - 3] + "..."
    else:
        x_text = base_summary
    x_text = f"{x_text} #AI {domain_suffix}"
    
    # Generate LinkedIn version (longer, more detailed)
    linkedin_text = f"""üì¢ {item.title}

{base_summary}

This article from {item.source_domain} discusses important developments in AI and technology.

What are your thoughts on this development? 

#AI #Technology #Innovation
{domain_suffix}"""
    
    if len(linkedin_text) > MAX_LINKEDIN_CHARS:
        linkedin_text = linkedin_text[:MAX_LINKEDIN_CHARS - 3] + "..."
    
    logger.info(f"Generated fallback summaries: X={len(x_text)} chars, LinkedIn={len(linkedin_text)} chars")
    
    return {
        "x_text": x_text,
        "linkedin_text": linkedin_text,
        "char_counts": {
            "x": len(x_text),
            "linkedin": len(linkedin_text)
        }
    }


def summarize_item(item: NewsItem) -> Dict[str, Any]:
    """
    Generate platform-specific summaries using OpenAI if available, otherwise fallback.
    Returns dict with x_text, linkedin_text, and char_counts.
    """
    domain = item.source_domain or "news"
    title = item.title.strip()
    link = item.link.strip()
    
    if OPENAI_API_KEY:
        try:
            logger.info("ü§ñ Using OpenAI for dual-platform summarization")
            return summarize_with_openai_dual(title, link, domain)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è OpenAI summarization failed, using fallback: {e}")
    
    logger.info("Using fallback summarization")
    return summarize_fallback(item)


# ----- Notion Integration -----
def notion_client() -> Client:
    """Create a Notion client instance."""
    logger.debug("Creating Notion client")
    if not NOTION_TOKEN:
        logger.error("NOTION_TOKEN environment variable is not set")
        raise RuntimeError("NOTION_TOKEN must be set")
    
    try:
        client = Client(auth=NOTION_TOKEN)
        logger.debug("Notion client created successfully")
        return client
    except Exception as e:
        logger.error(f"Failed to create Notion client: {e}", exc_info=True)
        raise


def notion_create_row(notion: Client, db_id: str, *, x_text: str, linkedin_text: str,
                      scheduled_time: datetime, media_url: Optional[str] = None,
                      status: str = "Scheduled", error: Optional[str] = None):
    """
    Create a row in the Notion database with platform-specific content.
    
    Args:
        x_text: Text for X (Twitter) post (‚â§280 chars)
        linkedin_text: Text for LinkedIn post (‚â§2000 chars)
    """
    logger.debug(f"notion_create_row() called - Status: {status}, X: {len(x_text)} chars, LinkedIn: {len(linkedin_text)} chars")
    
    # Use X text for Title (backward compatibility with existing workflow)
    properties = {
        "Title": {"title": [{"type": "text", "text": {"content": x_text}}]},
        "Scheduled Time": {"date": {"start": scheduled_time.replace(microsecond=0).isoformat().replace('+00:00', 'Z')}},
        "Status": {"select": {"name": status}},
        "X URL": {"url": None},
        "LinkedIn URL": {"url": None},
    }
    
    # Add LinkedIn text as rich_text property (for platform-specific posting)
    # Note: If "LinkedIn Text" property doesn't exist in Notion DB, this will be ignored
    # The main.py script will use Title for X and can optionally use LinkedIn Text property
    if linkedin_text and linkedin_text != x_text:
        properties["LinkedIn Text"] = {
            "rich_text": [{"type": "text", "text": {"content": linkedin_text[:2000]}}]  # Notion rich_text limit
        }
        logger.debug(f"Added LinkedIn-specific text property ({len(linkedin_text)} chars)")
    
    if media_url:
        properties["Media URL"] = {"url": media_url}
        logger.debug(f"Added media URL: {media_url}")
    
    if error:
        properties["Error Log"] = {"rich_text": [{"type": "text", "text": {"content": error[:1800]}}]}
        logger.debug(f"Added error log (truncated to 1800 chars)")
    
    logger.debug(f"Creating Notion page in database: {db_id[:8]}...")
    logger.debug(f"Properties: {list(properties.keys())}")
    
    try:
        response = notion.pages.create(parent={"database_id": db_id}, properties=properties)
        page_id = response.get("id", "unknown")
        logger.info(f"‚úÖ Notion page created successfully - ID: {page_id}")
        logger.debug(f"Notion response keys: {list(response.keys())}")
        return response
    except Exception as e:
        logger.error(f"‚ùå Failed to create Notion page: {type(e).__name__}: {e}", exc_info=True)
        raise


def write_skipped_row():
    """Write a Skipped row to Notion when no fresh items are found."""
    try:
        notion = notion_client()
        db_id = os.environ["NOTION_DB_ID"]
        skipped_text = "(No fresh AI news today.) (system)"
        notion_create_row(
            notion, db_id,
            x_text=skipped_text,
            linkedin_text=skipped_text,
            scheduled_time=datetime.now(timezone.utc) - timedelta(minutes=5),
            status="Skipped",
        )
        logger.info("Wrote Skipped row to Notion.")
    except Exception as e:
        logger.error("Failed to write Skipped row: %s", e)


def create_notion_entry(summaries: Dict[str, Any], item: NewsItem, dry_run: bool = False) -> bool:
    """
    Create a Notion database entry with Status=Scheduled and platform-specific content.
    
    Args:
        summaries: Dict with x_text, linkedin_text, and char_counts
        item: NewsItem object with article metadata
        dry_run: If True, skip actual Notion API call
    """
    if dry_run:
        logger.info("üß™ DRY RUN: Skipping Notion entry creation")
        logger.info(f"   üê¶ X text ({summaries['char_counts']['x']} chars): {summaries['x_text']}")
        logger.info(f"   üíº LinkedIn text ({summaries['char_counts']['linkedin']} chars):")
        logger.info(f"      {summaries['linkedin_text'][:200]}...")
        return True
    
    if not NOTION_TOKEN or not NOTION_DB_ID:
        raise RuntimeError("NOTION_TOKEN and NOTION_DB_ID must be set")
    
    scheduled_time = datetime.now(timezone.utc) - timedelta(minutes=5)
    
    try:
        notion = notion_client()
        notion_create_row(
            notion, NOTION_DB_ID,
            x_text=summaries["x_text"],
            linkedin_text=summaries["linkedin_text"],
            scheduled_time=scheduled_time,
            media_url=item.image_url,
            status="Scheduled",
        )
        logger.info(f"‚úÖ Created Notion entry for: {item.title[:50]}...")
        logger.info(f"   üìä Char counts - X: {summaries['char_counts']['x']}, LinkedIn: {summaries['char_counts']['linkedin']}")
        return True
    except Exception as e:
        logger.error(f"‚ùå Failed to create Notion entry: {e}")
        # Try to create error entry
        try:
            notion = notion_client()
            error_text = f"[ERROR] Failed to create entry for: {item.title[:100]}"
            notion_create_row(
                notion, NOTION_DB_ID,
                x_text=error_text,
                linkedin_text=error_text,
                scheduled_time=scheduled_time,
                status="Failed",
                error=str(e),
            )
            logger.info("Created error entry in Notion")
        except Exception as e2:
            logger.error(f"Failed to create error entry: {e2}")
        return False


# ----- Main -----
def main():
    parser = argparse.ArgumentParser(description="AI Content Fetcher")
    parser.add_argument("--dry-run", action="store_true", help="Print summary without writing to Notion")
    args = parser.parse_args()
    
    logger.info("=== AI Content Fetcher Started ===")
    logger.info(f"Dry run mode: {args.dry_run}")
    
    try:
        # 1. Parse feeds (with Notion duplicate checking)
        items = parse_feeds()
        
        if not items:
            logger.warning("No items found in any feed")
            if not args.dry_run:
                write_skipped_row()
            return 0
        
        # Get recent Notion content for scoring
        notion_recent = set()
        if NOTION_TOKEN and NOTION_DB_ID:
            try:
                notion = notion_client()
                notion_recent = get_recent_notion_content(notion, NOTION_DB_ID, days=7)
            except Exception as e:
                logger.warning(f"Could not fetch Notion history for scoring: {e}")
        
        # 2. Score and filter with enhanced diversity and freshness checks
        scored_items = score_items(items, notion_recent=notion_recent)
        
        if not scored_items:
            logger.warning(f"No items within last {MAX_ARTICLE_AGE_HOURS}h")
            if args.dry_run:
                print("No fresh items (‚â§48h); Skipped.")
                return 0
            write_skipped_row()
            print("No fresh items (‚â§48h); Skipped.")
            return 0
        
        # 3. Pick top item
        top_item = scored_items[0]
        logger.info(f"Selected top item (score={top_item.score:.2f}): {top_item.title}")
        logger.info(f"  Link: {top_item.link}")
        logger.info(f"  Published: {top_item.published}")
        logger.info(f"  Source: {top_item.source_domain}")
        
        # 4. Summarize with platform-specific content
        summaries = summarize_item(top_item)
        logger.info(f"üìù Generated platform-specific summaries:")
        logger.info(f"   üê¶ X: {summaries['char_counts']['x']} chars")
        logger.info(f"   üíº LinkedIn: {summaries['char_counts']['linkedin']} chars")
        
        # 5. Dry-run output
        if args.dry_run:
            print(json.dumps({
                "x_text": summaries["x_text"],
                "linkedin_text": summaries["linkedin_text"],
                "char_counts": summaries["char_counts"],
                "title": top_item.title,
                "link": top_item.link,
                "published": top_item.published.isoformat() if top_item.published else None,
                "image_url": top_item.image_url,
                "domain": top_item.source_domain,
                "note": "dry-run: Notion write skipped"
            }, ensure_ascii=False, indent=2))
            return 0
        
        # 6. Create Notion entry
        success = create_notion_entry(summaries, top_item, dry_run=args.dry_run)
        
        if success:
            logger.info("=== AI Content Fetcher Completed Successfully ===")
        else:
            logger.error("=== AI Content Fetcher Completed with Errors ===")
            sys.exit(1)
    
    except Exception as e:
        logger.exception("Fatal error in AI Content Fetcher")
        
        # Try to log error to Notion
        if not args.dry_run and NOTION_TOKEN and NOTION_DB_ID:
            try:
                notion = Client(auth=NOTION_TOKEN)
                scheduled_iso = datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace('+00:00', 'Z')
                error_text = "[ERROR] AI Content Fetcher failed"
                notion.pages.create(
                    parent={"database_id": NOTION_DB_ID},
                    properties={
                        "Title": {
                            "title": [{"text": {"content": error_text}}]
                        },
                        "Status": {"select": {"name": "Failed"}},
                        "Error Log": {"rich_text": [{"text": {"content": str(e)[:1800]}}]},
                        "Scheduled Time": {"date": {"start": scheduled_iso}},
                        "X URL": {"url": None},
                        "LinkedIn URL": {"url": None},
                    }
                )
            except:
                pass
        
        sys.exit(1)


if __name__ == "__main__":
    main()
